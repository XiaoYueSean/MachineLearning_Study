
## 基础术语
 - 数据集：所有记录的集合为
 - 一个实例（instance）或样本（sample）：数据集里面对一条数据
 - 特征（feature）、属性（attribute）：比如一个人的性别或年龄、职业等
 - 特征向量：对于一条记录，如果在坐标轴上表示，每个人都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（男，25岁，NLPer），即每个人可以被定义为为：一个特征向量（feature vector）。
 - 一个样本的特征数为：维数（dimensionality），该人的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。

	> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个***“训练样本”***，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个***“测试样本”***。定义：	

 - 所有训练样本的集合为：训练集（trainning set），[特殊]。
 - 所有测试样本的集合为：测试集（test set），[一般]。  
 - 机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。    
  
在人的例子中，我们是想计算机通过学习人的特征数据，训练出一个决策模型，来判断这个人是否秃顶。可以得知我们预测的是：这个人不秃顶，即`秃顶`和`不秃顶`两种，是离散值。同样地，也有通过这三个值来预测这个人的收入情况，人的收入值是连续值。定义：	

 - 预测值为离散值的问题为：分类（classification）。
 - 预测值为连续值的问题为：回归（regression）。

## 模型评估与选择
**误差与过拟合**

我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：	

 - 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。
 - 在测试集上的误差称为测试误差（test error）。
 - 学习器在所有新样本上的误差称为泛化误差（generalization error）。

显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：

 - 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。
 - 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。

可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。

![](https://i.loli.net/2018/10/17/5bc7181172996.png)

## 评估方法
评估目的：对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。

**留出法**（可能结果不稳定）  
将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：***训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。*** 同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

> 分层抽样法也叫类型抽样法。它是从一个可以分成不同子总体（或称为层）的总体中，按规定的比例从不同层中随机抽取样品（个体）的方法。这种方法的优点是，样本的代表性比较好，抽样误差比较小。缺点是抽样手续较简单随机抽样还要繁杂些。定量调查中的分层抽样是一种卓越的概率抽样方式，在调查中经常被使用。
> 
**交叉验证法**（最常用）

将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪...∪Dk，Di∩Dj=∅（i≠j），***同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。***交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。

![](https://i.loli.net/2018/10/17/5bc718115d224.png)

**自助法**(数据集小时常用)
自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。可以得知在m次采样中，样本始终不被采到的概率取极限为：

![](https://i.loli.net/2018/10/17/5bc71811246dd.png)

这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D'中，于是可以将D'作为训练集，D-D'作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。

**调参**： 大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)。