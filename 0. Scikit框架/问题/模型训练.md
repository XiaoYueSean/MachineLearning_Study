# 练习题

1. 如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？

- 如果训练集大于内存，可以用随机梯度下降或者小批量梯度下降。
- 如果训练集小于内存，可以用批量梯度下降。
- 一般是不能用标准方程法或者SVD，因为随着特征数量增加，复杂度会快速增长（超过n2）



2. 假设你训练集中特征的数值尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？

当特征具有不同尺寸比例，成本函数就会具有细长碗状当形状，梯度下降算法可能需要很长时间来收敛。你需要在训练之前将数据缩放（标准化）# 标准方程 or SVD 无需缩放也可以正常使用。

数据没有缩放对正则化模型有很大的影响，正则化模型可能会收敛到次优解。（因为正则化模型会惩罚较大的权重，因此与具有较大值的特征相比，具有较小的特征往往会被忽略）

> 正则化解释：regularizer，它是向模型中加入一些规则，加入先验，缩小解空间，减小求出错误解的可能性。你要把你的知识数学化告诉这个模型，对代价函数来说，就是加入对模型“长相”的惩罚

为什么正则化后，权重小的损失更多，而权重大的损失少

https://blog.csdn.net/qq_40378390/article/details/82954735

L1、L2正则化

https://zhuanlan.zhihu.com/p/68488202



3. 训练 Logistic 回归模型时，梯度下降是否会陷入局部最低点？
不会陷入局部最优，因为LR的损失函数是凸函数。


4. 在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？
常用的梯度下降算法有，批量梯度下降，随机梯度下降，小批量梯度下降。 如果Cost function 是凸的（如线性回归模型和逻辑回归），且学习率不是很高，那么梯度下降算法都将接近全剧最优并产生相似的模型。 但是，除非逐步降低学习率，随机梯度下降和小批量梯度下降将永远不会真正收敛。它们会围绕着全局最优点来回跳动。这也意味着会运行很多时间，也会产生略微不同的模型。

5. 假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？
验证集错误在每个轮次后上升，可能是学习率太高且算法在发散。
+ 如果训练集错误也是在上升，应该降低学习率
+ 如果训练机错误率降低或者没有什么变化，说明模型可能过拟合训练机，应立即停止训练
> 一种正则化优化方法“完美的免费午餐”，当验证集错误达到最低的时候停止训练所生成的模型，是最佳模型。

7. 当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？
   由于随机性，随机梯度下降和小批量梯度下降都不能保证每次在每次训练迭代中都有进展。因此，如果在验证错误上升都情况下停止训练，可能会在达到最优解之前就停止训练了。（更好都选择是按照一定的间隔时间保存模型。当很长时间没有改善时，就可以回复到保存的最佳模型）

8. 哪个梯度下降算法（在我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？
   随机梯度下降算法有最快的训练迭代速度（一次训练一个实例）。实际上只有批量梯度下降会收敛，随机梯度下降和小批量梯度下降会在最优解附近反弹，除非你逐步降低学习率。

9.  假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？
    如果是验证误差远高于学习误差，说明模型过拟合了。解决的办法有：降低多项式阶数；将模型正则化（l1，l2惩罚添加到cost function）；增加训练数据

10. 假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数 ，还是降低它？  
    误差都很高说明欠拟合训练集，这意味着模型有很高的偏差，应该尝试降低正则化超参数a（即减少约束，让模型复杂）

11. 为什么要使用：
- 使用岭回归代替线性回归？  
  通常正则化的模型比没有任何正则化的模型要好。
- Lasso 回归代替岭回归？  
  Lasso 回归使用l1 惩罚，这通常会将权重降低为0，这将导致稀疏模型。其中除了重要权重之外，所有权重都为0. 这是一种自动进行特征选择的方法
- 弹性网络代替 Lasso 回归？


12. 假设你想判断一副图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器，还是一个 Softmax 分类器？
   两个逻辑回归，分别对应 室内/室外； 白天/晚上。

13. 在 Softmax 回归上应用批量梯度下降的早期停止法（不使用 Scikit-Learn）。