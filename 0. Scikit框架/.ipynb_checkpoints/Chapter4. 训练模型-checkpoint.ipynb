{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "我将以一个简单的线性回归模型为例，讨论两种不同的训练方法来得到模型的最优解：\n",
    "\n",
    "1. 直接使用封闭方程进行求根运算，得到模型在当前训练集上的最优参数（即在训练集上使损失函数达到最小值的模型参数）\n",
    "\n",
    "2. 使用迭代优化方法：梯度下降（GD），在训练集上，它可以逐渐调整模型参数以获得最小的损失函数，最终，参数会收敛到和第一种方法相同的的值。\n",
    "\n",
    "明确概念：啥叫训练模型？\n",
    "> 训练模型就是通过设置模型的参数，直到模型最拟合训练集的过程。  \n",
    "但首先要知道怎么测量模型对训练数据的拟合程度是好是坏。（定义cost-function）\n",
    "\n",
    "## 标正规方程求解(the normal equation)\n",
    "对于线性回归模型，解为\n",
    "$$\\hat{\\theta} = ({\\mathbf{X}}^T\\cdot\\mathbf{X})^{-1}\\cdot{\\mathbf{X}}^T\\cdot\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:14:44.954423Z",
     "start_time": "2021-03-27T14:14:44.936860Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X] # 添加 X0 = 1 给每个实例\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:14:52.462421Z",
     "start_time": "2021-03-27T14:14:52.458393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88044941],\n",
       "       [3.04436274]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "梯度下降是一种非常通用的优化算法，它能够很好地解决一系列问题。梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。\n",
    "\n",
    "假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量$\\theta$的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。\n",
    "\n",
    "具体来说，开始时需要选定一个随机的$\\theta$（这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到损失函数收敛到一个最小值\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} M S E(\\theta)=\\frac{2}{m} \\sum_{i=1}^{m}\\left(\\theta^{T} \\cdot \\mathbf{x}^{(i)}-y^{(i)}\\right) x_{j}^{(i)}\n",
    "$$\n",
    "\n",
    "### 批量梯度下降\n",
    "\n",
    "使用梯度下降的过程中，你需要计算每一个 $\\theta_j$ 下损失函数的梯度。换句话说，你需要计算当$\\theta_j$变化一点点时，损失函数改变了多少。这称为偏导数，它就像当你面对东方的时候问：\"我脚下的坡度是多少？\"。然后面向北方的时候问同样的问题（如果你能想象一个超过三维的宇宙，可以对所有的方向都这样做）。公式 4-5 计算关于 $\\theta_j$ 的损失函数的偏导数，记为 $\\frac{\\partial }{\\partial \\theta_j}MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:42:02.817517Z",
     "start_time": "2021-03-27T09:42:02.814937Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:15:52.658372Z",
     "start_time": "2021-03-27T14:15:52.646492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批量梯度下降最优参数为\n",
      "[[3.88044888]\n",
      " [3.04436319]]\n"
     ]
    }
   ],
   "source": [
    "# 手推\n",
    "eta = 0.05\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # 初始化参数\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "print('批量梯度下降最优参数为\\n{}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在这个方程中每一步计算时都包含了整个训练集 $\\mathbf{X}$，这也是为什么这个算法称为批量梯度下降：每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢（但是我们接下来将会介绍更快的梯度下降算法）。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用*梯度下降要比使用正规方程快的多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***优点***：\n",
    "+ 可以计算出最优解\n",
    "+ 在小样本中，速度快\n",
    "\n",
    "***缺点***：\n",
    "+ 每一步梯度计算时需要使用整个训练集，导致在数据量大的情况下，计算会非常慢\n",
    "+ 如果cost function 不是凸函数，得到的解可能不是最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降\n",
    "批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。\n",
    "\n",
    "另一方面，由于它的随机性，与批量梯度下降相比，其呈现出更多的不规律性：它到达最小值不是平缓的下降，损失函数会忽高忽低，只是在大体上呈下降趋势。随着时间的推移，它会非常的靠近最小值，但是它不会停止在一个值上，它会一直在这个值附近摆动。因此，当算法停止的时候，最后的参数还不错，但不是最优值\n",
    "\n",
    "优点：\n",
    "+ 可以在大样本中使用，收敛的速度快\n",
    "+ 可以逃离局部最优\n",
    "+ 每次迭代只需要在内存运行一个少量数据  \n",
    "\n",
    "缺点：\n",
    "+ 定位不出最优解\n",
    "\n",
    "注意：\n",
    "随机梯度下降时，训练实例必须独立且均匀分布，以确保平均来说将参数拉向全局最优。确保这一点的简单方法就是训练过程中对实例进行随机混洗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:17:00.565596Z",
     "start_time": "2021-03-27T14:17:00.431458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机梯度下降最优参数为\n",
      "[[3.8866175 ]\n",
      " [3.04800728]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  #learning_schedule的超参数\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta)-yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "print('随机梯度下降最优参数为\\n{}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Scikit-Learn完成线性回归的随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:18:47.253403Z",
     "start_time": "2021-03-27T14:18:47.246509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:19:05.879509Z",
     "start_time": "2021-03-27T14:19:05.875093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.83597333]), array([3.06894473]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量梯度下降\n",
    "优点：\n",
    "+ 可以通过矩阵运算的硬件优化得到一个较好的训练表现\n",
    "+ 小批量梯度下降会比随机梯度更靠近最小值\n",
    "\n",
    "缺点：\n",
    "+ 很难摆脱局部最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T14:23:33.364251Z",
     "start_time": "2021-03-27T14:23:33.360452Z"
    }
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
